\Sexpr{set_parent('Lock5withR.Rnw')}

\setcounter{chapter}{3}
\Chapter{Hypothesis Tests}

\section{Introducing Hypothesis Tests}

\subsection*{The 4-step outline}
The following 4-step outline is a useful way to organize the ideas of hypothesis testing.

\begin{enumerate}
  \item
		State the Null and Alternative Hypotheses
	\item
		Compute the Test Statistic

		The test statistic is a number that summarizes the evidence 
	\item
		Determine the p-value (from the Randomization Distribution)
	\item
		Draw a conclusion
\end{enumerate}

\subsection*{Null and Alternative Hypotheses}

%Example 4.1
%Example 4.2
%Example 4.3

\subsubsection*{Figure 4.1}

<<Figure4.1>>=
gf_point(ZPenYds ~ NFL_Malevolence, data = MalevolentUniformsNFL) %>% 
  gf_lm(ZPenYds ~ NFL_Malevolence, data = MalevolentUniformsNFL)
@

%Example 4.4
%Example 4.5
%Example 4.6
%Example 4.7
%Example 4.8
%Example 4.9
%Example 4.10
%Example 4.11
%Example 4.12

\section{Measuring Evidence with P-values}

Randomization distributions are a bit like bootstrap distributions except that instead of resampling from our sample (in an attempt to approximate resampling from the population), we need to sample from a situation in which our null hypothesis is true.

\subsection*{P-values from Randomization Distributions}

\subsubsection*{Example 4.13}

Testing one proportion.
\begin{enumerate}
  \item
  	$H_0$: $p = 0.5$
    
    $H_a$: $p > 0.5$
	\item
		Test statistic:  $\hat p = 16/25$ (the sample proportion)
	\item
		We can simulate a world in which $p = 0.5$ using \function{rflip()}:
<<Example4.13, cache=TRUE>>=
Randomization.Match <- 
  do(10000) * rflip(25, .5)  # 25 because n = 25
head(Randomization.Match)
gf_histogram( ~ prop, binwidth = 0.04, data = Randomization.Match)
@

Here we find the proportion of the simulations which resulted in 16 or more matches out of 25, or 0.64 or greater, for the p-value.
<<Example4.13b>>=
prop( ~ (prop >= 0.64), data = Randomization.Match ) # 16/25
gf_histogram( ~ prop, binwidth = 0.04, fill = ~(prop >= 0.64), 
              data = Randomization.Match, show.legend = FALSE)
@

<<Figure4.7, include=FALSE>>=
<<Example4.13b>>
@
\end{enumerate}

%Example 4.14

\subsubsection*{Example 4.15}

<<Example4.15>>=
prop( ~ (prop >= 0.60), data = Randomization.Match ) # 15/25
prop( ~ (prop >= 0.76), data = Randomization.Match ) # 19/25
gf_histogram( ~ prop, binwidth = 0.04, fill = ~(prop >= 0.60), data = Randomization.Match, show.legend = FALSE )
gf_histogram( ~ prop, binwidth = 0.04, fill = ~(prop >= 0.76), data = Randomization.Match, show.legend = FALSE )
@

<<Figure4.9, include=FALSE>>=
<<Example4.15>>
@

\subsubsection*{Example 4.16}

<<Example4.16>>=
prop( ~ (prop >= 0.88), data = Randomization.Match ) # 22/25
histogram( ~ prop, width = 0.04, v = c(0.88), data = Randomization.Match )
gf_histogram(~ prop, binwidth = 0.04, data = Randomization.Match )
@
\authNote{not sure how to make that vertical line}

%Example 4.17

\subsubsection*{Figure 4.10}

<<Figure4.10, opts.label="fig1">>=
gf_dotplot(~ Taps, binwidth = 1, dotsize = .3, data = CaffeineTaps) %>%
  gf_facet_grid(Group ~ .)
@


\subsubsection*{Example 4.18}

Testing two means.
<<Example4.18>>=
mean(Taps ~ Group, data = CaffeineTaps)
diff( mean(Taps ~ Group, data = CaffeineTaps) )
@

\begin{enumerate}
  \item
    $H_0$: $\mu_1 = \mu_2$
    
    $H_a$: $\mu_1 > \mu_2$
	\item
		Test statistic:  $\bar x_1 - \bar x_2 = 3.5$ (the difference in sample means)
	\item
		We simulate a world in which $\mu_1 = \mu_2$ or $\mu_1 - \mu_2 = 0$:
<<Example4.18b, cache=TRUE>>=
Randomization.Caff <- do (1000) * ediff( mean( Taps ~ shuffle(Group), data = CaffeineTaps ) )
head(Randomization.Caff,3)
gf_dotplot( ~ No.Caffeine, binwidth = 0.1, dotsize = .7, data = Randomization.Caff)
@

<<Example4.18c>>=
prop( ~ (No.Caffeine >= 3.5), data = Randomization.Caff )
gf_dotplot( ~ No.Caffeine, binwidth = 0.1, fill = ~(No.Caffeine >=3.5), 
            data = Randomization.Caff, show.legend = FALSE)
@
\authNote{there shouldn't be a legend because I used show.legend = FALSE}
<<Figure4.11, include=FALSE>>=
<<Example4.18c>>
@
\end{enumerate}

\subsection*{P-values and the Alternative Hypothesis}

\subsubsection*{Example 4.19}

Testing one proportion.
\begin{enumerate}
  \item
    $H_0$: $p = 0.5$
    
    $H_a$: $p > 0.5$
	\item
		Test statistic:  $\hat p = 0.8, 0.6, 0.4$ (the sample proportion of 8/10, 6/10, 4/10 heads)
	\item
		We simulate a world in which $p = 0.5$:
<<Example4.19, seed=12345, cache=TRUE>>=
RandomizationDist <- do(1000) * rflip(10, .5)  # 10 because n = 10
head(RandomizationDist)
histogram( ~ prop, label = TRUE, width = 1/10, data = RandomizationDist)
gf_dhistogram( ~ prop, binwidth = 1/10, data = RandomizationDist) %>% gf_labs(label = TRUE)
@
\authNote{how do you get labels in the graph}
<<Figure4.12,include=FALSE>>=
<<Example4.19>>
@
<<Example4.19b>>=
prop( ~ (prop >= 0.8), data = RandomizationDist )
prop( ~ (prop >= 0.6), data = RandomizationDist )
prop( ~ (prop >= 0.4), data = RandomizationDist )
@
\end{enumerate}

\subsubsection*{Example 4.20}

Testing one proportion.
\begin{enumerate}
  \item
    $H_0$: $p = 0.5$
    
    $H_a$: $p \neq 0.5$
  \item
		Test statistic:  $\hat p = 0.8$ (the sample proportion of 8/10 heads)
	\item
		We use the simulated world in which $p = 0.5$:
<<Example4.20, tidy=FALSE>>=
prop(~ (prop >= 0.8), data = RandomizationDist)
prop(~ (prop <= 0.2), data = RandomizationDist)
@

<<Example4.20b>>=
# a 2-sided p-value is the sum of the values above
prop(~ (prop <= 0.2 | prop >= 0.8), data = RandomizationDist)
# We can also approximate the p-value by doubling one side
2 * prop(~ prop >= 0.80, data = RandomizationDist)
@
%Why wrong histogram?

<<Figure4.13,include=FALSE>>=
<<Example4.20>>
@
\end{enumerate}

\section{Determining Statisical Significance}

%Example 4.21
%Example 4.22
%Example 4.23
%Example 4.24
%Example 4.25
%Example 4.26

\subsection*{Less Formal Statistical Decisions}

\subsubsection*{Example 4.27}

Testing two means.
<<Example4.27>>=
head(Smiles)
mean(Leniency ~ Group, data = Smiles)
diffmean(Leniency ~ Group, data = Smiles)
@

\begin{enumerate}
  \item
    $H_0$: $\mu_1 = \mu_2$
    
    $H_a$: $\mu_1 \neq \mu_2$
  \item
  	Test statistic:  $\bar x_1 - \bar x_2 = 0.79$ (the difference in sample means)
	\item
		We simulate a world in which $\mu_1 = \mu_2$:
<<Example4.27b, cache=TRUE>>=
Randomization.Smiles <- do(1000) * diffmean(Leniency ~ shuffle(Group), data = Smiles)
head(Randomization.Smiles, 3)
@
<<Example4.27c, tidy=FALSE>>=
prop(~ (diffmean <= -0.79 | diffmean >= 0.79), data = Randomization.Smiles)
2 * prop(~ diffmean >= 0.79, data = Randomization.Smiles )
gf_dotplot(~ diffmean, binwidth = 0.03, dotsize = 1, fill = ~(diffmean >= 0.79), 
        xlab = "Diff", data = Randomization.Smiles, show.legend = FALSE)
@
\authNote{show.legend issue}
<<Figure4.24,include=FALSE>>=
<<Example4.27c>>
@

Now we find the p-value:
<<Example4.27d>>=
prop( ~ (diffmean <= -0.79 | diffmean >= 0.79), data = Randomization.Smiles)
2 * prop( ~ diffmean >= 0.79, data = Randomization.Smiles )
gf_dotplot( ~ diffmean, binwidth = .03, dotsize = 1, fill = ~(diffmean >= 0.79), 
            data = Randomization.Smiles, show.legend = FALSE)
@
\authNote{show.legend issue}
\end{enumerate}

\section{Creating Randomization Distributions}

In order to use these methods to estimate a p-value, we must be able to generate a randomization distribution. In the case of a test with null hypothesis claiming that a proportion has a particular value (e.g, $H_0$: $p=0.5$), this is pretty easy.  If the population has proportion 0.50, we can simulate sampling from that proportion by flipping a fair coin.  If the proportion is some value other than 0.50, we simply flip a coin that has the appropriate probability of resulting in heads.  So the general template for creating such a randomization distribution is

<<eval=FALSE>>=
do(1000) * rflip( n, hypothesized_proportion )
@
where \texttt{n} is the size of the original sample.

In other situations, it can be more challenging to create a randomization distribution because the null hypothesis does not directly specify all of the information needed to simulate samples.
\begin{itemize}
	\item
		$H_0$: $p_1 = p_2$

		This would be simple \emph{if} we new the value of $p_1$ and $p_2$ (we could use \function{rflip()} twice, once
		for each group), 
	\item
		$H_0$: $\mu = $ some number

		Just knowing the mean does not tell us enough about the distribution.
		We need to know about its shape.  (We might need to know the standard
		deviation, for example, or whether the distribution is skewed.)

	\item
		$H_0$: $\mu_1 \neq \mu_2$ some number.

		Now we don't know the common mean and we don't know the things mentioned in the previous example either.
\end{itemize}

So how do we come up with randomization distribution?

\begin{boxedText}
	The main criteria to consider when creating randomization samples for a statistical test are:
	\begin{itemize}
		\item
			Be consistent with the null hypothesis.

			If we don't do this, we won't be testing our null hypothesis.
		\item
			Use the data in the original sample.

			With luck, the original data will shed light on some aspects of the distribution
			that are not determined by null hypothesis.
		\item
			Reflect the way the original data were collected.
	\end{itemize}
\end{boxedText}

\subsection*{Randomization Test for a Difference in Proportions: Cocaine Addiction}

\subsubsection*{Data 4.7} 
Data 4.7 in the text describes some data that are not in a data frame.  This often happens when a data set has only categorical variables because a simple table completely describes the distributions involved.  Here's the table from the book:\footnote{The book includes data on an additional treatment group which we are omitting here.}

\begin{center}
  \begin{tabular}{ccc}
  	\hline
		 & Relapse & No Relapse 
		 \\ \hline
%		 Desipramine & 10 & 14 \\
		 Lithium & 18 & 6 \\
		 Placebo & 20 & 4 \\
		 \hline
	 \end{tabular}
\end{center}

Here's one way to create the data in \R:
%  do(10) * data.frame( treatment = "Desipramine", response="Relapse"),
%  do(14) * data.frame( treatment = "Desipramine", response="No Relapse"),
<<Section4.4b,tidy=FALSE>>=
Cocaine <- rbind( 
  do(18) * data.frame( treatment = "Lithium", response = "Relapse"),
  do(6)  * data.frame( treatment = "Lithium", response = "No Relapse"),
  do(20) * data.frame( treatment = "Placebo", response = "Relapse"),
  do(4)  * data.frame( treatment = "Placebo", response = "No Relapse")
  )
@

%Example 4.28

\subsubsection*{Example 4.29}

Testing two proportions.
<<Example4.29>>=
tally( response ~ treatment, data = Cocaine )
prop( response ~ treatment,  data = Cocaine )
diff( prop( response ~ treatment, data = Cocaine ))
@

\begin{enumerate}
  \item
    $H_0$: $p_1 = p_2$
    
    $H_a$: $p_1 < p_2$
	\item
		Test statistic:  $\hat p_1 = \hat p_2$ (the difference in sample proportions)
	\item
		We simulate a world in which $p_1 = p_2$ or $p_1 - p_2 = 0$:
<<Example4.29b, cache=TRUE>>=
Randomization.Coc <- do (5000) * diff( prop( response ~ shuffle(treatment), data = Cocaine ) )
head(Randomization.Coc)
@

<<Example4.29c>>=
prop( ~ (Relapse.Placebo < -0.0833 ), data = Randomization.Coc )
gf_histogram( ~ Relapse.Placebo, data = Randomization.Coc, v = c(-0.0833), binwidth = 0.08)
@
\authNote{need to make line at -0.0833}
<<Figure4.28, include=FALSE>>=
<<Example4.29c>>
@
\end{enumerate}

\subsection*{Randomization Test for a Correlation: Malevolent Uniforms and Penalties}

%Example 4.30

\subsubsection*{Example 4.31}

Testing correlation.
<<Example4.31>>=
xyplot(ZPenYds ~ NFL_Malevolence, type = c("p", "r"), data = MalevolentUniformsNFL)
gf_point(ZPenYds ~ NFL_Malevolence, data = MalevolentUniformsNFL) %>% 
  gf_lm(ZPenYds ~ NFL_Malevolence, data = MalevolentUniformsNFL)
cor(ZPenYds ~ NFL_Malevolence, data = MalevolentUniformsNFL)
@

\begin{enumerate}
  \item
    $H_0$: $\rho = 0$
    
    $H_a$: $\rho > 0$
	\item
		Test statistic:  $r = 0.43$ (the sample correlation)
	\item
		We simulate a world in which $\rho = 0$:
<<Example4.31b, cache=TRUE, tidy=FALSE>>=
Randomization.Mal <- 
  do(10000) * cor(NFL_Malevolence ~ shuffle(ZPenYds), data = MalevolentUniformsNFL)
head(Randomization.Mal)
@

<<Example4.32c>>=
prop( ~ (cor > 0.43 ), data = Randomization.Mal )
histogram(~ cor, v = c(0.43), width = 0.05, data = Randomization.Mal)
@

<<Figure4.29, include=FALSE>>=
<<Example4.32c>>
@
\end{enumerate}

\subsection*{Randomization Test for a Mean: Body Temperature}

%Example 4.32

\subsubsection*{Example 4.33}

Testing one mean.
<<Example4.33, opts.label="fig1">>=
mean( ~ BodyTemp, data = BodyTemp50)
dotPlot( ~ BodyTemp, v = c(98.26), width = .1, cex = .2, data = BodyTemp50)
@
<<Figure4.30,include=FALSE>>=
<<Example4.33>>
@

\begin{enumerate}
  \item
    $H_0$: $\mu = 98.6$
    
    $H_a$: $\mu \neq 98.6$
  \item
		Test statistic:  $\bar x = 98.26$ (the sample mean)
    
Notice that the test statistic differs a bit from 98.6
<<Example4.33b>>=
98.6 - mean( ~ BodyTemp, data = BodyTemp50)  
@

But might this just be random variation?
We need a randomization distribution to compare against.
    
	\item
		If we resample, the mean will not be 98.6.  But we shift the distribution a bit, then we will
have the desired mean while preserving the shape of the distribution indicated by
our sample. We simulate a world in which $\mu = 98.6$:
<<Example4.33c, seed=1234, cache=TRUE>>=
Randomization.Temp <- 
  do(10000) * ( mean( ~ BodyTemp, data = resample(BodyTemp50) ) + 0.34 ) 
head(Randomization.Temp, 3)
mean( ~ result, data = Randomization.Temp )
cdata( ~ result, 0.95, data = Randomization.Temp )
@
From this we can estimate the p-value:
<<Example4.33d, opts.label="fig4">>=
prop( ~ abs(result - 98.6) > 0.34, data = Randomization.Temp)
histogram( ~ result, width = .01, v = c(98.40, 98.6, 98.81), data = Randomization.Temp)
@

<<Figure4.31,include=FALSE>>=
<<Example4.33d>>
@
\end{enumerate}

How do we interpret this (estimated) p-value of 0?  Is it impossible to have a sample mean so 
far from 98.6 if the true population mean is 98.6?  No.  This merely means that we didn't see any
such cases \emph{in our 10000 randomization samples}.   
We might estimate the p-value as $p < 0.001$. Generally, to more accurately estimate small p-values, we must use many more randomization samples.

\subsubsection*{Example 4.33: A different approach}

An equivalent way to do the preceding test is based on a different way of expressing our 
hypotheses.
\begin{enumerate}
  \item
    $H_0$: $\mu - 98.6 = 0$
    
    $H_a$: $\mu - 98.6 \neq 0$
  \item
		Test statistic:  $\bar x - 98.6 = -0.34$
	\item
		We we create a randomization distribution centered at $\mu - 98.6 = 0$:
<<Example4.33e, cache=TRUE>>=
Randomization.Temp2 <- 
  do(5000) * ( mean( ~ BodyTemp, data = resample(BodyTemp50) ) - 98.26 ) 
head(Randomization.Temp2, 3)
mean( ~ result, data = Randomization.Temp2 )
@
From this we can estimate the p-value:
<<Example4.33f, , opts.label="fig4">>=
prop( ~ abs(result) > 0.34, data = Randomization.Temp2 )
histogram( ~ result, width = .01, v = c(0.34, -0.34), data = Randomization.Temp2)
@
\end{enumerate}

Often there are multiple ways to express the same hypothesis test.

%Example 4.34

\section{Confidence Intervals and Hypothesis Tests}

If your randomization distribution is centered at the wrong value, then it isn't simulating a world in which the null hypothesis is true.  This would happen, for example, if we got confused about randomization vs. bootstrapping.

\subsection*{Randomization and Bootstrap Distributions}

\subsubsection*{Figure 4.32}

<<Figure4.32, seed=1234, cache=TRUE,opts.label="fig4">>=
Boot.Temp <- do (5000) * mean(~ BodyTemp, data = resample(BodyTemp50))
head(Boot.Temp,3)
mean(~ mean, data = Boot.Temp)
cdata( ~ mean, 0.95, data = Boot.Temp)
histogram( ~ mean, width = .01, v = c(98.26, 98.6), 
          groups = (98.05 <= mean & mean <= 98.46), data = Boot.Temp)
@
Notice that the distribution is now centered at our test statistic instead of at the value
from the null hypothesis.

\subsubsection*{Example 4.35}

\begin{enumerate}
  \item
    $H_0$: $\mu = 98.4$
    
    $H_a$: $\mu \neq 98.4$
  \item
  	Test statistic:  $\bar x = 98.26$ (the sample mean)
	\item
		We simulate a world in which $\mu = 98.4$:
<<Example4.35, seed=1234, cache=TRUE, opts.label="fig4">>=
Randomization.Temp3 <- 
  do(5000) * ( mean( ~ BodyTemp, data = resample(BodyTemp50) ) + 0.14 ) 
head(Randomization.Temp3, 3)
mean( ~ result, data = Randomization.Temp3 )
cdata(~ result, 0.95, data = Randomization.Temp3)
histogram( ~ result, width = .01, v = c(98.26, 98.4), 
          groups = (98.19 <= result & result <= 98.62), 
          xlim = c(97.8, 99.0), data = Randomization.Temp3) # randomization
histogram( ~ mean, width = .01, v = c(98.26, 98.4), 
          groups = (98.05 <= mean & mean <= 98.46), 
          xlim = c(97.8, 99.0), data = Boot.Temp) # bootstrap
@

\end{enumerate}

%Example 4.36
%Example 4.37
%Example 4.38
%Example 4.39
%Example 4.40

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\iffalse
\subsubsection{An Observational Study: ?? }

Once again, we are interested in comparing two proportions:
\begin{itemize}
	\item $p_t = $ the proportion of people who lost a ticket that say they would buy a new one.
	\item $p_c = $ the proportion of people who lost cash that say they would buy a ticket.
\end{itemize}
And our null hypothesis is again 
\begin{itemize}
	\item $H_0$:  $p_t = p_c$ 
\end{itemize}
But the design of this study is different.

\subsection{Testing Whether Two Means are Equal}

\subsubsection{Random assignment}
The procedure for comparing two means is very similar

<<>>=
mean(BodyTemp ~ Sex, data = BodyTemp50)
diffmean(BodyTemp ~ Sex, data = BodyTemp50) 
@

<<>>=
Randomization.Temp <- 
  do (1000) * diffmean( BodyTemp ~ shuffle(Sex), data = BodyTemp50)
head(Randomization.Temp,3)
histogram( ~ diffmean, data = Randomization.Temp, v = c(0.176, -0.176))
@
<<>>=
prop( ~ (diffmean > 0.176), data = Randomization.Temp )       # 1-sided
2 * prop( ~ (diffmean > 0.176), data = Randomization.Temp )   # 2-sided
prop( ~ (abs(diffmean) > 0.176), data = Randomization.Temp )  # 2-sided
@

\subsubsection{Separate sampling from two groups}
But wait; one of our criteria was that we our randomization should mimic the design of the study.
In this case, we did not randomly assign the sex of the subjects (for obvious reasons).  Is there
a randomization method that more accurately reflects what actually happened?  It turns out there is.
We could 
\begin{itemize}
	\item
		First pool the 50 subjects' body temperatures.  
		
		(If men and women aren't different, then
		they really form one population with the same distribution rather than two different 
		distributions, so we can safely combine in this way when the null hypothesis is true.)

	\item
		Then randomly draw two samples of size 25 with replacement to create simulated
		groups of women and men.
\end{itemize}
Here's a way to do this in \R:

<<>>=
Randomization.Temp2 <- 
  do (1000) * diffmean( resample(BodyTemp) ~ shuffle(Sex), data = BodyTemp50 )
head(Randomization.Temp2, 3)
histogram( ~ diffmean, data = Randomization.Temp2, v = c(0.176, -0.176))
@

<<>>=
prop( ~ (diffmean > 0.176), data = Randomization.Temp2 )       # 1-sided
2 * prop( ~ (diffmean > 0.176), data = Randomization.Temp2 )   # 2-sided
prop( ~ (abs(diffmean) > 0.176), data = Randomization.Temp2 )  # 2-sided
@

\subsubsection{Sampling from one population with two variables}

If on the other hand our study design was to randomly sample 50 people and record their 
body temperature and their sex, then another approach could be used:

<<>>=
Randomization.Temp3 <- 
  do (1000) * diffmean( BodyTemp ~ shuffle(Sex), data = resample(BodyTemp50))
head(Randomization.Temp3, 3)
histogram( ~ diffmean, data = Randomization.Temp3, v = c(0.176, -0.176))
@

<<>>=
prop( ~ (diffmean > 0.176), data = Randomization.Temp3 )       # 1-sided
2 * prop( ~ (diffmean > 0.176), data = Randomization.Temp3 )   # 2-sided
prop( ~ (abs(diffmean) > 0.176), data = Randomization.Temp3 )  # 2-sided
@

As we see, the three methods give essentially the same results in this case.  But they correspond to
three different study designs:
\begin{enumerate}
	\item
		Randomized assignment of treatments (shuffle the treatment)
	\item
		Separate sampling from two populations (resample the response, shuffle the treatment)

		Each randomization sample will have the same number of cases in each group.

	\item
		Observational study on two variables (resample entire data set, and shuffle the 
		explanatory variable)

		Different randomization samples will have different numbers of cases in the two groups.
\end{enumerate}
Similar approaches work for testing whether two proportions are equal as well.


Take a look at the three histograms in this section.  Notice how each is centered at 0.   (They
also have our hoped for bell shape.)
<<>>=
mean( ~ diffmean, data = Randomization.Temp )
mean( ~ diffmean, data = Randomization.Temp2 )
mean( ~ diffmean, data = Randomization.Temp3 )
@
This is because we are computing the difference in means \emph{assuming the difference in means is 0}.

\fi