```{r include=FALSE}
require(mosaic)
require(mosaicData)
require(ggformula)
library(Lock5withR)
require("fastR")
```
`r chapter <- 8`
# Inference for Regression

## Inference for Slope and Correlation

### Simple Linear Model {-}

\[
Y = \beta_0 + \beta_1 x + \epsilon  \qquad \mbox{where $\epsilon \sim Norm(0, \sigma)$.}
\]

In other words:
<span class="itemize">

#. The mean response for a given predictor value $x$ is given by a linear formula
  \[
  \mbox{mean response} = \beta_0 + \beta_1 x
  \]

#. The distribution of all responses for a given predictor value $x$ is normal.
#. The standard deviation of the responses is the same for each predictor value.

</span>


One of the goals in simple linear regression is to estimate this linear relationship -- that
is to estimate the intercept and the slope.

Of course, there are lots of lines.
We want to determine the line that fits the data best. But what does that mean?

The usual method is called the **method of least squares** and  chooses the line that has the 
_ smallest possible sum of squares of residuals_, where residuals are defined by

\[
\mbox{residual} = \mbox{observed response} - \mbox{predicted response}
\]

For a line with equation $y = b_0 + b_1 x$, this would be 
\[
e_i = y_i - (b_0 + b_1 x) 
\]

Simple calculus (that you don't need to know) allows us to compute the best
$b_0$ and $b_1$ possible.  These best values define the least squares regression line.
Fortunately, statistical software packages do all this work for us.  In R 
the command that does this is `lm()`.

#### Example 9.1 {-}

```{r Example9.1}
lm(Price ~ PPM, data = InkjetPrinters)
```

```{r include = FALSE}
ink.coefs <- do(1) * lm(Price ~ PPM, data = InkjetPrinters)
```

You can get terser output with
```{r Example9.1b}
coef(lm(Price ~ PPM, data = InkjetPrinters))
```

You can also get more information with
```{r Example9.1c}
msummary(lm(Price ~ PPM, data = InkjetPrinters))
```

So our regression equation is 

\[
\widehat{\mbox{Price}} = `r ink.coefs$Intercept` + `r ink.coefs$PPM` \cdot \mbox{PPM}
\]

For example, this suggests that the average price for inkjet printers that print 3 pages per minute is
\[
\widehat{\mbox{Price}} 
= 
`r ink.coefs$Intercept` + `r ink.coefs$PPM` \cdot \mbox{3.0}
=
`r ink.coefs$Intercept + ink.coefs$PPM * 3.0`
\]

### Inference for Slope {-}

#### Figure 9.1 {-}
```{r Figure9.1}
gf_point(Price ~ PPM, data = InkjetPrinters) %>% gf_lm()
```


#### Figure 9.2 {-}

```{r Figure9.2}
Boot.Ink <- do(1000) * lm(Price ~ PPM, data = resample(InkjetPrinters))
favstats( ~ PPM, data = Boot.Ink)
gf_dotplot( ~ PPM, binwidth = 2, data = Boot.Ink)
Rand.Ink <- do(1000) * lm(Price ~ shuffle(PPM), data = InkjetPrinters)
favstats( ~ PPM, data = Rand.Ink)
gf_dotplot( ~ PPM, binwidth = 2, data = Rand.Ink)
```

<!-- how to get just the slope? -->

#### Example 9.2 {-}

```{r Example9.2}
msummary(lm(Price ~ PPM, data = InkjetPrinters)) 
confint(lm(Price ~ PPM, data = InkjetPrinters) , "PPM")
```


#### Example 9.3 {-}

```{r Example9.3}
head(RestaurantTips)
summary(lm(Tip ~ Bill, data = RestaurantTips)) 
confint(lm(Tip ~ Bill, data = RestaurantTips) , "Bill", level = 0.90)
```


#### Example 9.4 {-}

<span class="enumerate">

#. $H_0$: $\beta_1 = 0$;  $H_a$: $\beta_1 \neq 0$
#. Test statistic:  $b_1 = 0.0488$ (sample slope)
#. t-test for slope:

```{r Example9.4}
msummary(lm(PctTip ~ Bill, data = RestaurantTips)) 
```


</span>


### t-Test for Correlation {-}

#### Example 9.5 {-}

```{r Example9.5}
summary(lm(CostBW ~ PPM, data = InkjetPrinters)) 
```

<!-- how to do test for correlation? -->

#### Example 9.6 {-}

```{r Example9.6}
msummary(lm(PctTip ~ Bill, data = RestaurantTips)) 
```


### Coefficient of Determination: R-squared {-}

#### Example 9.7 {-}

```{r Example9.7}
msummary(lm(Price ~ PPM, data = InkjetPrinters))
```


### Checking Conditions for a Simple Linear Model {-}

<!-- #### Figure 9.3 {-} -->

<!-- #### Figure 9.4 {-} -->

<!-- #### Example 9.8 {-} -->

<!-- #### Figure 9.5 {-} -->

#### Example 9.9 {-}

```{r Example9.9}
gf_point(Tip ~ Bill, data = RestaurantTips, cex = 0.5) %>% gf_lm()
gf_point(PctTip ~ Bill, data = RestaurantTips, cex = 0.5) %>% gf_lm()
```



## ANOVA for Regression

### Partitioning Variability {-}

We can also think about regression as a way to analyze the variability in the response. This is a lot like the ANOVA tables we have seen before.  This time:

<span class="align*">

$SST$ = $\sum (y - \overline{y})^2$  

$SSE$ = $\sum (y - \hat{y})^2$  

$SSM$ = $\sum (\hat{y} - \overline{y})^2$  

$SST$ = $SSM$ + $SSE$

</span>

As before, when $SSM$ is large and $SSE$ is small, then the model ($\hat y = \hat\beta_0 + \hat\beta_1 x$) explains a lot of the variability and little is left unexplained ($SSE$).  On the other hand, if $SSM$ is small and $SSE$ is large, then the model explains only a little of the variability and most of it is due to things not explained by the model.

#### Example 9.10 {-}

```{r Example9.10}
summary(lm(Calories ~ Sugars, data = Cereal))
anova(lm(Calories ~ Sugars, data = Cereal))
```


### F-Statistic {-}

<!-- \def\fit#1{#1} -->
<!-- \def\resid#1{#1} -->

<span class="itemize">


* $MSM = SSM / DFM = SSM / (number of groups - 1)$
* $MSE = SSE / DFE = SSE / (n - number of groups)$

</span>

MS stands for "mean square"

Our test statistic is 
\[
F = \frac{MSM}{MSE} 
\]
<!-- AuthNote --- it seems that fit and resid are undefined and so I'm unable to compile the pdf -->

#### Example 9.11 {-}

```{r Example9.11}
SSM <- 15317
MSM <- SSM / (2-1); MSM
SSE <- 19834
MSE <- SSE / (30-2); MSE
```


```{r Example9.11b}
F <- MSM / MSE; F
pf(F, 1, 28, lower.tail = FALSE)
```

<!-- any way to get total sum of squares? -->

#### Example 9.12 {-}

```{r Example9.12}
summary(lm(Calories ~ Sodium, data = Cereal))
anova(lm(Calories ~ Sodium, data = Cereal))
```


The percentage of explained variability is denoted $r^2$ or $R^2$:

\[
R^2 = \frac{SSM}{SST} = \frac{SSM}{SSM + SSE}
\]

#### Example 9.13 {-}

The summary of the linear model shows us the **coefficient of determination** but we can also find it manually.

```{r Example9.13}
SSM <- 15317
SST <- SSM + 19834
R2 <- SSM / SST; R2
rsquared(lm(Calories ~ Sugars, data = Cereal))
```


```{r Example9.13b}
SSM <- 3241
SST <- SSM + 31909
R2 <- SSM / SST; R2
rsquared(lm(Calories ~ Sodium, data = Cereal))
```


### Computational Details {-}
<!-- Example 9.14 -->

#### Example 9.15 {-}

Again, the summary of the linear model gives us the standard deviation of the error but we can calculate it manually.

```{r Example9.15}
SSE <- 31909
SD  <- sqrt(SSE / (30 - 2)); SD
```


#### Example 9.16 {-}

```{r Example9.16}
favstats( ~ Sodium, data = Cereal)
SE <- SD / (77.4 * sqrt(30 - 1)) # SD from Example 9.15
SE
```



## Confidence and Prediction Intervals

### Interpreting Confidence and Prediction Intervals {-}

It may be very interesting to make predictions when the explanatory variable has some other value, however. There are two ways to do this in R   One uses the `predict()` function.  It is simpler, however, to use the `makeFun()` function in the **`mosaic`** package, so that's the approach we will use here.

Prediction intervals 
<span class="enumerate">

#. are much wider than confidence intervals
#. are very sensitive to the assumption that the population normal for each value of the predictor.
#. are (for a 95\% confidence level) a little bit wider than 
  \[
  \hat y \pm 2 SE
  \]
  where $SE$ is the ``residual standard error'' reported in the summary output.
  <span class="enumerate">

#. The prediction interval is a little wider because it takes into account the uncertainty in our estimated slope and intercept as well as the variability of responses around the true regression line.

</span>


</span>


<!-- Example 9.17 -->


#### Example 9.18 {-}

First, let's build our linear model and store it.
```{r Example9.18}
ink.model <- lm(Price ~ PPM, data = InkjetPrinters)
summary(ink.model)
```


Now let's create a function that will estimate values of <span style="color:green">Price</span> for a given
value of <span style="color:green">PPM</span>:
```{r Example9.18b}
Ink.Price <- makeFun(ink.model)
```

We can now input a PPM and see what our least squares regression line predicts for the price:
```{r Example9.18c}
Ink.Price(PPM = 3.0)   # estimate Price when PPM is 3.0
```


R  can compute two kinds of confidence intervals for the response for a given value
<span class="enumerate">

#. A confidence interval for the _mean response_ for  a _given explanatory value_ can be computed by adding interval = 'confidence'. 

    ```{r Example9.18d}
    Ink.Price(PPM = 3.0, interval = 'confidence')
    ```

#. An interval for an _individual response_ (called a prediction interval to avoid confusion with the confidence interval above) can be computed by adding interval = 'prediction'! instead. 

    ```{r Example9.18e}
    Ink.Price(PPM = 3.0, interval = 'prediction')
    ```


</span>


#### Figure 9.13 {-}

The figure below shows the confidence (inner band) and prediction (outer band) intervals as bands around the regression line.  
```{r Figure9.13}
gf_point(Price ~ PPM, data = InkjetPrinters,
       dotsize = .6, alpha = .5) %>% 
  gf_lm() %>% 
  gf_lm(interval = "confidence") %>% 
  gf_lm(interval = "prediction")
```

As the graph illustrates, the intervals are narrow near the center of the data and wider near the edges of the data. It is not safe to extrapolate beyond the data (without additional information), since there is no data to let us know whether the pattern of the data extends.

<!-- Example 9.19 -->

<!-- ## Inference (Confidence Intervals and Hypothesis Tests) -->

<!-- This section is currently Unused and has been Deleted-->
<!-- Look back in the source control for this section's code -->