`r chapter <- 6`
```{r include=FALSE}
require(Lock5withR)
require(mosaic)
require(ggformula)
require("fastR")
```

# Chi-Squared Tests for Categorical Variables

<!-- do(10) *  chisq.test(tally( ~ resample(toupper(letters[1:5]), 400) ) )$statistic -->
<!-- rmultinom(10, 400, prob = c(.2, .2, .2, .2, .2)) -->

Goodness of fit tests test how well a distribution fits some hypothesis.

## Testing Goodness-of-Fit for a Single Categorical Variable

#### Example 7.1 {-}

```{r Example7.1}
tally(~Answer, format = "proportion", data = APMultipleChoice)
```


<!-- ### Null and Alternative Hypotheses {-} -->
<!-- Example 7.2 -->

<!-- ### Expected Counts {-} -->
<!-- Example 7.3 -->
<!-- Example 7.4 -->

### Chi-square Statistic {-}


----

The **Chi-squared test statistic**:
\[
\chi^2 = \sum \frac{(\mbox{observed} - \mbox{expected})^2 }{ \mbox{expected} }
\]
There is one term in this sum _for each cell in our data table_, and

<span class="itemize">

* observed $=$ the tally in that cell (a count from our raw data)

* expected $=$ the number we would "expect" if the percentages followed our null hypothesis exactly.  (Note: the expected counts might not be whole numbers.)

</span>

----


#### Example 7.5 {-}

You could calculate the chi-square statistic manually but of course, R  can automate this whole process for us if we provide the data table and the null hypothesis. Notice that to use `chisq.test()`, you must enter the data like answer <- c(85, 90, 79, 78, 68). The default null hypothesis is that all the probabilities are equal.

```{r Example7.5}
head(APMultipleChoice)
answer <- c(85, 90, 79, 78, 68)
chisq.test(answer)
```


<!-- #### Figure 7.1 {-} -->

### Chi-square Distribution {-}

The `chisq()` function can be used to calculate a chi-squared statistic from one 
of three kinds of input: from the result of `chisq.test()`, from a table of counts, as 
produced by `tally()`, or from a formula and data frame that could have been the input
to `tally()`.

```{r }
chisq(sex ~ substance, data = HELPrct)
chisq(tally(sex ~ substance, data = HELPrct))
chisq(chisq.test(tally(sex ~ substance, data = HELPrct)))
```


#### Figure 7.2 {-}

```{r Figure7.02}
chisq.sample <- do(1000) *  chisq( ~ resample(toupper(letters[1:5]), 400))
gf_dhistogram(~X.squared, bins = 11, data = chisq.sample)
```


#### Figure 7.3 {-}

```{r Figure7.03, fig.keep = 'last'}
gf_dist("chisq", params = list(df = 4), lty = 1) %>%
  gf_vline(xintercept = 3.425)
```

<!-- AuthNote --- using vline instead of coloring sections, add label for at 3.424 -->
Our test statistic will be large when the observed counts and expected counts are quite different.  It will be small when the observed counts and expected counts are quite close.  So we will reject when the test statistic is large. To know how large is large enough, we need to know the sampling distribution.


----

If $H_0$ is true and the sample is large enough, then the sampling distribution for the Chi-squared test statistic will be approximately a Chi-squared distribution.  

<span class="itemize">

  * The **degrees of freedom** for this type of goodness of fit test is one less than the number of cells.

  * The approximation gets better and better as the sample size gets larger.

</span>

----


The mean of a Chi-squared distribution is equal to its degrees of freedom. This can help us get a rough idea about whether our test statistic is unusually large or not.

#### Example 7.6 {-}

<span class="enumerate">
  
#. $H_0$: $p_w= 0.54$, $p_b= 0.18$, $p_h= 0.12$, $p_a= 0.15$, $p_o= 0.01$;  
  
    $H_a$: At least one $p_i$ is not as specified.
  
#. Observed count: $w=780$, $b=117$, $h=114$, $a=384$, $o=58$
#. Chi-squared test:

```{r Example7.6}
jury <- c(780, 117, 114, 384, 58)
chisq.test(jury, p = c(.54, .18, .12, .15, .01))
xchisq.test(jury, p = c(.54, .18, .12, .15, .01)) # to list expected counts
```

Notice in this example, we need to tell R  what the null hypothesis is.

How unusual is it to get a test statistic at least as large as 
ours? We compare to a Chi-squared distribution with 4 degrees of freedom. The mean value of such a statistic is 4, and our test statistic is much larger, so we anticipate that our value is extremely unusual.
</span>

<!-- Example 7.7 -->

### Goodness-of-Fit for Two Categories {-}

When there are only two categories, the Chi-squared goodeness of fit test is equivalent to the 1-proportion test. Notice that `prop.test()` uses the count in one category and total but that `chisq.test()` uses cell counts.

#### Example 7.8 {-}

```{r Example7.8}
prop.test(84, 200)
chisq.test(c(84, 116), p = c(.5, .5))
binom.test(84, 200)
```


Although all three tests test the same hypotheses and give similar p-values (in this
example), the binomial test is generally used because
<span class="itemize">
#. The binomial test is exact for all sample sizes while the Chi-squared test and 1-proportion test
are only approximate, and the approximation is poor when sample sizes are small.
#.   The binomial test and 1-proportion test also produce confidence intervals.
</span>


## Testing for an Association Between Two Categorical Variables

#### Example 7.9 {-}

```{r Example7.9}
OneTrueLove <- read.file("OneTrueLove.csv")
tally(~Response, format = "proportion", data = OneTrueLove)
tally(~ Response + Gender, format = "proportion", margin = TRUE, data = OneTrueLove)
```


#### Figure 7.4 {-}

```{r Figure7.04}
gf_bar(~Response|Gender, data = OneTrueLove)
```


<!-- ### Null and Alternative Hypothesis {-} -->
<!-- ### Expected Counts for a Two-Way Table {-} -->

### Chi-square Test for Association {-}

#### Example 7.10 {-}

```{r Example7.10}
head(WaterTaste, 3)
water <- tally(~UsuallyDrink + First, data = WaterTaste); water
```

```{r Example7.10b}
water <- rbind(c(14, 15, 8, 4), c(11, 26, 16, 6)) # to combine Tap and Filtered
water
colnames(water) <- c('Aquafina', 'Fiji', 'SamsChoice', 'Tap') # add column names
rownames(water) <- c('Bottled', 'Tap/Filtered') # add row names
water
```

```{r Example7.10c, warning = FALSE}
xchisq.test(water)
```


### Special Case for a 2 x 2 Table {-}

There is also an exact test that works only in the case of a $2Ã—$ table (much like the binomial test can be used instead of a goodness of fit test if there are only two categories).  The test is called **Fisher's Exact Test**.

In this case we see that the simulated p-value from the Chi-squared Test is nearly the same as the exact p-value from Fisher's Exact Test.  This is because Fisher's test is using mathematical formulas to compute probabilities of _all_ randomizations -- it is essentially the same as doing infinitely many randomizations!  

Note: For a $2Ã—$ table, we could also use the method of 2-proportions (`prop.test()`, manual resampling, or formula-based).  The approximations based on the normal distribution will be poor in the same situations where the Chi-squared test gives a poor approximation.

#### Example 7.11 {-}
```{r Example7.11}
SplitStealTable <- rbind(c(187, 195), c(116, 76)); SplitStealTable
colnames(SplitStealTable) <- c('Split', 'Steal')
rownames(SplitStealTable) <- c('Younger', 'Older'); SplitStealTable
```

```{r Example7.11b}
fisher.test(SplitStealTable)
xchisq.test(SplitStealTable)
```

To use the test for proportions as done in Example 6.23,
```{r Example7.11c, tidy = FALSE}
SplitStealData <- rbind( 
  do(187) * data.frame(agegroup = "Under40", decision = "Split"),
  do(195) * data.frame(agegroup = "Under40", decision = "Steal"),
  do(116) * data.frame(agegroup = "Over40",  decision = "Split"),
  do(76)  * data.frame(agegroup = "Over40",  decision = "Steal")
  )
```

```{r Example7.11d}
prop.test(decision ~ agegroup, data = SplitStealData)
```

<!-- ## Goodness of Fit Testing for Simple Hypotheses -->
<!-- This section is currently Unused and has been Deleted-->
<!-- Look back in the source control for this section's code -->

